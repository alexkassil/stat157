{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Homework 8 - Berkeley STAT 157\n",
    "\n",
    "**Your name: XX, SID YY, teammates A,B,C** (Please add your name, SID and teammates to ease Ryan and Rachel to grade.)\n",
    "\n",
    "**Please submit your homework through [gradescope](http://gradescope.com/)**\n",
    "\n",
    "Handout 4/9/2019, due 4/16/2019 by 4pm.\n",
    "\n",
    "This homework deals with sequence models for text and numbers. Due to the computational cost, we strongly encourage you to implement this on a GPU enabled machine. To make things a bit more interesting we will use a larger text collection here - [Shakespeare's collected works](http://www.gutenberg.org/files/100/100-0.txt) which are freely downloadable at Project Gutenberg. \n",
    "\n",
    "**This is teamwork.**\n",
    "\n",
    "## Prerequisites - Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mxnet-cu92\n",
      "^C\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "import collections\n",
    "import re\n",
    "shakespeare = 'http://www.gutenberg.org/files/100/100-0.txt'\n",
    "\n",
    "http = urllib3.PoolManager()\n",
    "text = http.request('GET', shakespeare).data.decode('utf-8')\n",
    "raw_dataset = ' '.join(re.sub('[^A-Za-z]+', ' ', text).lower().split())\n",
    "\n",
    "print('number of characters: ', len(raw_dataset))\n",
    "print(raw_dataset[0:70])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is quite a bit bigger than the time machine (5 million vs. 160k). For convenience we also include the remaining preprocessing steps. A bigger dataset will allow us to generate more meaningful models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_char = list(set(raw_dataset))\n",
    "char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "vocab_size = len(char_to_idx)\n",
    "corpus_indices = [char_to_idx[char] for char in raw_dataset]\n",
    "sample = corpus_indices[:20]\n",
    "print('chars:', ''.join([idx_to_char[idx] for idx in sample]))\n",
    "print('indices:', sample)\n",
    "\n",
    "train_indices = corpus_indices[:-100000]\n",
    "test_indices = corpus_indices[-100000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we import other useful libraries to help you getting started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d2l\n",
    "import math\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import loss as gloss, nn, rnn\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = d2l.try_gpu()\n",
    "ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens = 256\n",
    "rnn_layer = rnn.RNN(num_hiddens)\n",
    "rnn_layer.initialize()\n",
    "\n",
    "model = d2l.RNNModel(rnn_layer, vocab_size)\n",
    "model.initialize(force_reinit=True, ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train Recurrent Latent Variable Models\n",
    "\n",
    "Train a number of different latent variable models using `train_indices` to assess their performance. By default pick 256 dimensions for the hidden units. You can use the codes provided in the class. Also, we strongly encourage you to use the Gluon implementation since it's a lot faster than building it from scratch.\n",
    "\n",
    "1. Train a single-layer RNN (with latent variables). \n",
    "1. Train a single-layer GRU.\n",
    "1. Train a single-layer LSTM. \n",
    "1. Train a two-layer LSTM. \n",
    "\n",
    "How low can you drive the perplexity? Can you reproduce some of Shakespeare's finest writing (generate 200 characters). Start the sequence generator with `But Brutus is an honorable man`. Experiment with a number of settings:\n",
    "\n",
    "* Number of hidden units. \n",
    "* Embedding length.\n",
    "* Gradient clipping.\n",
    "* Number of iterations. \n",
    "* Learning rate.\n",
    "\n",
    "**Save** the models (at least in memory since you'll need them in the next exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class has been saved in the d2l package for future use\n",
    "class RNNModel(nn.Block):\n",
    "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dense = nn.Dense(vocab_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        # Get the one-hot vector representation by transposing the input to\n",
    "        # (num_steps, batch_size)\n",
    "        X = nd.one_hot(inputs.T, self.vocab_size)\n",
    "        Y, state = self.rnn(X, state)\n",
    "        # The fully connected layer will first change the shape of Y to\n",
    "        # (num_steps * batch_size, num_hiddens)\n",
    "        # Its output shape is (num_steps * batch_size, vocab_size)\n",
    "        output = self.dense(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)\n",
    "# This function is saved in the d2l package for future use\n",
    "def predict_rnn_gluon(prefix, num_chars, model, vocab_size, ctx, idx_to_char,\n",
    "                      char_to_idx):\n",
    "    # Use the model's member function to initialize the hidden state.\n",
    "    state = model.begin_state(batch_size=1, ctx=ctx)\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    for t in range(num_chars + len(prefix) - 1):\n",
    "        X = nd.array([output[-1]], ctx=ctx).reshape((1, 1))\n",
    "        # Forward computation does not require incoming model parameters\n",
    "        (Y, state) = model(X, state)\n",
    "        if t < len(prefix) - 1:\n",
    "            output.append(char_to_idx[prefix[t + 1]])\n",
    "        else:\n",
    "            output.append(int(Y.argmax(axis=1).asscalar()))\n",
    "    return ''.join([idx_to_char[i] for i in output])\n",
    "# This function is saved in the d2l package for future use\n",
    "def train_and_predict_rnn_gluon(model, num_hiddens, vocab_size, ctx,\n",
    "                                corpus_indices, idx_to_char, char_to_idx,\n",
    "                                num_epochs, num_steps, lr, clipping_theta,\n",
    "                                batch_size, pred_period, pred_len, prefixes):\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "    model.initialize(ctx=ctx, force_reinit=True, init=init.Normal(0.01))\n",
    "    trainer = gluon.Trainer(model.collect_params(), 'sgd',\n",
    "                            {'learning_rate': lr, 'momentum': 0, 'wd': 0})\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(epoch)\n",
    "        l_sum, n, start = 0.0, 0, time.time()\n",
    "        data_iter = d2l.data_iter_consecutive(\n",
    "            corpus_indices, batch_size, num_steps, ctx)\n",
    "        state = model.begin_state(batch_size=batch_size, ctx=ctx)\n",
    "        for X, Y in data_iter:\n",
    "            for s in state:\n",
    "                s.detach()\n",
    "            with autograd.record():\n",
    "                (output, state) = model(X, state)\n",
    "                y = Y.T.reshape((-1,))\n",
    "                l = loss(output, y).mean()\n",
    "            l.backward()\n",
    "            # Clip the gradient\n",
    "            params = [p.data() for p in model.collect_params().values()]\n",
    "            d2l.grad_clipping(params, clipping_theta, ctx)\n",
    "            # Since the error has already taken the mean, the gradient does\n",
    "            # not need to be averaged\n",
    "            trainer.step(1)\n",
    "            l_sum += l.asscalar() * y.size\n",
    "            n += y.size\n",
    "\n",
    "        if (epoch + 1) % pred_period == 0:\n",
    "            print('epoch %d, perplexity %f, time %.2f sec' % (\n",
    "                epoch + 1, math.exp(l_sum / n), time.time() - start))\n",
    "            for prefix in prefixes:\n",
    "                print(' -', predict_rnn_gluon(\n",
    "                    prefix, pred_len, model, vocab_size, ctx, idx_to_char,\n",
    "                    char_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.predict_rnn_gluon('but brutus is an honorable man', 10, model, vocab_size, ctx, idx_to_char,\n",
    "                  char_to_idx)\n",
    "\n",
    "num_epochs, batch_size, lr, clipping_theta = 500, 32, 10, 1e-2\n",
    "pred_period, pred_len, prefixes = 50, 50, ['but brutus is an honorable man']\n",
    "train_and_predict_rnn_gluon(model, num_hiddens, vocab_size, ctx,\n",
    "                            corpus_indices, idx_to_char, char_to_idx,\n",
    "                            num_epochs, 30, lr, clipping_theta,\n",
    "                            batch_size, pred_period, pred_len, prefixes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    },
    "collapsed": true
   },
   "source": [
    "## 2. Test Error\n",
    "\n",
    "So far we measured perplexity only on the training set. \n",
    "\n",
    "1. Implement a perplexity calculator that does not involve training.\n",
    "1. Compute the perplexity of the best models in each of the 4 categories on the test set. By how much does it differ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. $N$-Gram Model\n",
    "\n",
    "So far we only considered latent variable models. Let's see what happens if we use a regular $N$-gram model and an autoregressive setting. That is, we aim to predict the next character given the current characters one character at a time. For this implement the following:\n",
    "\n",
    "1. Split data into $(x,y)$ pairs as before, just that we now use very short subsequences, e.g. only $5$ characters. That is, `But Brutus` turns into the tuples (`(But B, r)`, `(ut Br, u)`, `(t Bru, t)`, `( Brut, u)`, `(Brutu, s)`). \n",
    "1. Use one-hot encoding for each character separately and combine them all. \n",
    "    * In one case use a sequential encoding to obtain an embedding proportional to the length of the sequence.\n",
    "    * Use a bag of characters encoding that sums over all occurrences.\n",
    "1. Implement an MLP with one hidden layer and 256 hidden units.\n",
    "1. Train it to output the next character.\n",
    "\n",
    "How accurate is the model? How does the number of operations and weights compare to an RNN, a GRU and an LSTM discussed above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
